---
title: "Assignment 1"
output:
  pdf_document: default
  html_notebook: default
---

_**Group 3**: Sophie Engels, Boyan Mihaylov and Jonas SchÃ¤fer_   
_Experimental Design and Data Analysis, VU Amsterdam (2024)_

## Exercise 1. Ice Cream

**a)** A first statistical analysis of the data set `Ice_cream.csv` is performed below:
```{r, fig.height=2.4, fig.width=6, fig.align = "center"}
#Read in the data and extract the relevant column
ice_cream=read.csv("data/Ice_cream.csv",header=TRUE) 
video=ice_cream$video

#Create relevant figures to assess distribution normality
par(mfrow=c(1,3))
hist(video, xlab="Video Game Scores", main="Scores Histogram"); 
boxplot(video, ylab="Video Game Scores", main="Scores Boxplot"); 
qqnorm(video, main="Video Scores Q-Q Plot")
```

The data for the video games scores appears to be close to normally distributed. Both the histogram and boxplot indicate that the scores are unimodal and moderately evenly distributed about the mean. The qq-plot confirms this as the data points are nearly linear. Hence, any testing or methods used with a normality assumption are justified.

```{r}
#Constructing a bounded 97% confidence interval 
video_mean = mean(video)
video_se = sd(video)/sqrt(length(video))
alpha = 0.03
degrees_freedom = length(video) - 1
t_score = qt(p=alpha/2, df=degrees_freedom, lower.tail=F)
margin_error = video_se * t_score
lower_bound = video_mean - margin_error; upper_bound = video_mean + margin_error
```

The bounds for the 97% confidence interval are [`r round(lower_bound, 4)`, `r round(upper_bound, 4)`].

```{r}
#Calculating the number of samples needed for a confidence interval of at most length 3
desired_margin_error = 1.5
z_score = qnorm(0.985)
num_samples = ((z_score*sd(video))/desired_margin_error)^2
```

The number of samples needed for a confidence interval of at most length 3 is at least `r ceiling(num_samples)`.

```{r}
#Creating a bootstrapped 97% confidence interval
B = 1000
Tstar = numeric(B)
for (i in 1:B) {
  Xstar = sample(video, replace = TRUE) 
  Tstar[i] = mean(Xstar)
}
Tstar015 = quantile(Tstar, 0.015)  #Lower bound
Tstar985 = quantile(Tstar, 0.985)  #Upper bound

confidence_interval = c(2*mean(video) - Tstar985, 2*mean(video) - Tstar015)
```

Our bootstrapped confidence interval is (`r round(confidence_interval[[1]], 4)`, `r round(confidence_interval[[2]], 4)`). We notice that this confidence interval is minutely smaller than the bounded confidence interval above. However, they are extremely similar. Since the data is already relatively normally distributed, the original confidence interval likely provides a good enough estimate and bootstrapping is unnecessary in this case.

**b)** The following t-tests are performed:
```{r}
#First t-test when mu_0 = 50
t.test(video, mu=50, alt="g")

#Second t-test when mu_0 = 51
t.test(video, mu=51, alt="g")
```

Since the p-value for the first one-sided t-test when $\mu_{0}$ = 50 is less than 0.05, we would reject the null hypothesis that the mean video game score for the sample is equal to 50 in favor of the alternative hypothesis that it is greater than 50. Since we are doing a right-sided t-test, we have a confidence interval that extends to the left of the mean by the standard error amount, but to the right to infinity, since we do not actually specify or care what the upper bound is for the mean, as long as it is greater than 50. In the second case, we have the same case with regard to the confidence interval. However, our t-statistic and p-value have changed. In the calculation of the t-value, we subtract $\mu$ from our calculated mean. When t differs greatly from 0, our p-value will be larger. In this case, our p-value is greater than 0.05, so we do not reject the null hypothesis that the mean is equal to 51. We also notice that our actual mean is closer to 51 than it is to 50, so intuitively we would expect that we're more likely to accept the alternative hypothesis that the mean is larger than 50 than the alternative that the mean is larger than 51. 

**c)** Several tests are performed:
```{r}
binom.test(sum(video>50), length(video), p=0.5, alt="g")

wilcox.test(video, mu=50, alt="g")

binom.test(sum(video<42), length(video), p=0.25, alt="l")
```

First, we conduct a sign test to determine whether or not the median is greater than 50. Our null hypothesis is that the median is less than or equal to fifty, while our alternative hypothesis is that it is greater than 50. Since our resulting p-value is greater than 0.05, we do not reject the null hypothesis that the median is less than or equal to 50. This test does not align with out result from part b); however, in this case we are comparing medians and not means. In particular, with the sign test, we are not comparing the actual data values. Therefore, t-test is more sensitive to smaller deviations from the mean and any slight skewness. 

Secondly, we conduct a Wilcoxon signed rank test. Since our data is close to normal, we fulfill the requirement of a symmetric distribution. We follow the same hypotheses as the sign test above. However, since our p-value is much less than 0.05, we reject the null hypothesis in favor of the alternative.

To perform a test to check whether the fraction of the scores less than 42 is at most 25%, we can do a modified version of the sign test for medians, but instead compare the lower quartiles. In this case, we have a null hypothesis that the lower quartile is greater than or equal to 42 and an alternative hypothesis that it is less than or equal to 42. Since our p-value is much lower than 0.05, we reject the null hypothesis in favor of the alternative that in fact the fraction of scores less than 42 is at most 25%.

**d)** The following bootstrap test is performed:
```{r}
n=length(video); t=min(video)
mus = c()
B=1000; tstar=numeric(B)
for (j in 0:100){
  for (i in 1:B){
  xstar=rnorm(200, mean=j, sd=10)
  tstar[i]=min(xstar)
  }
  pl=sum(tstar<t)/B; pr=sum(tstar>t)/B
  p=2*min(pl,pr)
  if (p >= 0.05){
    mus = c(mus, j)
  }
}
```

The values of $\mu$ for which the null hypothesis is not rejected are `r mus`.

```{r}
ks_mus = c()
suppressWarnings(for (j in 0:100){
  xstar=rnorm(200, mean=j, sd=10)
  p = ks.test(video, xstar)[[2]]
  if (p >= 0.05){
    ks_mus = c(ks_mus, j)
  }
})
```

The Kolmogorov-Smirnov test is applicable in this case since we have two independent samples: one from our dataset and one from our simulated normal distribution. In this case, we can conduct an experiment similar to that above. By using a Kolmogorov-Smirnov test, the values of $\mu$ for which the null hypothesis is not rejected are `r ks_mus`.

**e)** To verify the claims that the mean video game score is higher for male students compared to female students, three tests are conducted:
```{r, fig.height=3.2, fig.width=6, fig.align = "center"}
#Extracting the boolean data for scores from women and men
women_scores = ice_cream$video[ice_cream$female == 1]
men_scores = ice_cream$video[ice_cream$female == 0]

par(mfrow=c(1,2))
qqnorm(women_scores, main="Women's Scores Q-Q Plot")
qqnorm(men_scores, main="Men's Scores Q-Q Plot")

#Two-sample t-test
t.test(women_scores, men_scores)
```

In order to conduct a two-sample t-test, we assume independence in our samples. In this case that is applicable, since we assume each player plays the game without confounding factors. We also assume normality in the data. According to our qq-plots, we see that the data points are almost linear, so we can carefully make this assumption. The results of the t-test indicate a p-value that is greater than 0.05, so we do not reject the null hypothesis that the mean values of the video game scores for men and women are different.

```{r}
#Mann-Whitney test
wilcox.test(women_scores, men_scores)

#Kolmogorov-Smirnov test
ks.test(women_scores, men_scores)
```

Similarly, the Mann-Whitney and Kolmogorov Smirnov tests assume independence of samples. In this case, both of these are applicable. In both cases, we find p-values that are greater than 0.05. For these tests, we do not reject the null hypothesis that the underlying distributions of these samples are different. In this case, the permutation test is not applicable because that requires paired samples, which these are not.

**f)** Next, the correlation between the columns `video` and `puzzle` is examined:
```{r, fig.height=3.6, fig.width=3.6, fig.align = "center"}
puzzle=ice_cream$puzzle

#Check normality for the puzzle scores
qqnorm(puzzle, main="Puzzle Scores Q-Q Plot")
cor.test(video, puzzle, method="spearman")
```

To use Pearson's correlation test, we need to first check the normality assumption for the puzzle score data, since we have already checked the assumption for the video game score data. The data looks more like a step function than a linear function, so the normality assumption is doubtful. It is then safer to use Spearman's correlation test, which does not assume normality. In this case, the null hypothesis is that the correlation coefficient is equal to 0, i.e. there is no correlation. Since our p-value for the Spearman test is much lower than 0.05, we reject the null hypothesis in favor of the alternative, that there is a significant correlation between the two samples.


## Exercise 2. Hemoglobin in trout

In this exercise, the influence of varying amounts of the antibacterial sulfamerazine on the hemoglobin levels in the blood of brown trout is explored. The statistical analysis is performed on the data set `hemoglobin.txt`:

```{r}
hg_data = read.table("data/hemoglobin.txt", header=TRUE)
mu <- mean(hg_data$hemoglobin)
summary(hg_data)
```

The set consists of 80 entries. The administering of the treatment is conducted using two different methods, A and B, and in four types of doses, labeled 1, 2, 3 and 4 and corresponding to 0, 5, 10 and 15 g of the drug. The rate (dose) and method are the factors which will be analysed, and should therefore be transformed into factor datatypes.

```{r}
hg_data$rate = as.factor(hg_data$rate); hg_data$method = as.factor(hg_data$method)
```

**a)** Upon first inspection of the data, it could be identified that the sample in the data set exhibits *balanced design*, i.e. with an equal amount of observations per factor level combination. This can be verified by creating a contingency table of the counts at each factor level combination:

```{r}
table(hg_data$rate, hg_data$method)
```

It is evident that each factor combination contains $N=10$ experimental units. This makes the data suitable for a two-way ANOVA test on the factors `rate` and `method`. The code below simulates how such a balanced-design sample can be acquired from a large fish population. For this purpose, a population of $M=1000$ fish is generated, each assigned a random `rate` and `method` factor, and a hemoglobin level sampled from a normal distribution with a mean and a standard deviation equivalent to those of the original data set. The resulting data set is then sampled to obtain a balanced design of 80 fish with $N=10$ units per combination, and the contingency table is calculated to verify the balance of the sample.

```{r, fig.height = 3.2, fig.width = 6, fig.align = "center"}
set.seed(123)
M=1000; I=4; J=2

# Generate population
rates <- sample(1:I, M, replace=TRUE); methods <- sample(c('A', 'B'), M, replace=TRUE)
hemoglobin <- rnorm(M, mean(hg_data$hemoglobin), sd(hg_data$hemoglobin))
dummy_pop <- data.frame(rate=rates, method=methods, hemoglobin=hemoglobin)

# Create factor combinations
N=10
dummy_pop$combos <- interaction(dummy_pop$rate, dummy_pop$method)
combo_groups <- split(dummy_pop, dummy_pop$combos)

# Sample balanced design
balanced_groups <- lapply(combo_groups,
                          function(group) group[sample(nrow(group),min(N, nrow(group))), ])
balanced_data <- do.call(rbind, balanced_groups)
balanced_data <- subset(balanced_data, select=-combos)

# Verify balance
table(balanced_data$rate, balanced_data$method)
```

**b)** Using the provided data set, a two-way ANOVA test is performed to investigate the influence of the factors `rate` and `method` on the hemoglobin levels in the trout. The test is conducted by constructing a linear model from the two factors and using it as input for the `anova` function. By default, a treatment parameterization is used.

```{r}
model <- lm(hemoglobin~rate*method, data=hg_data); anova(model)
```

The first observation to be made is whether the $H_{AB}$ hypothesis holds, i.e. whether an interaction between the two factors is absent. This is done by examining the p-value of the interaction term in the ANOVA table. In this case, $p=0.3769$, which is not significant at the 5% level, and thus the null hypothesis is not rejected. This suggests that there is no evidence for interaction between the two factors. Considering the separate effect of the factors, the p-value for the `rate` factor is below the significance level, indicating a rejection of the null-hypothesis, while the p-value of the `method` factor is above the significance level, indicating a failure to reject the null-hypothesis.

**c)** To validate the test, the assumptions of the ANOVA test should be checked. The residuals of the model are examined for normality using a Q-Q-plot and are plotted against the fitted model values to ensure there is no pattern in the residuals.

```{r fig.height = 3.2, fig.width = 6, fig.align = "center"}
par(mfrow=c(1,2))
qqnorm(residuals(model)); plot(fitted(model), residuals(model)); title("Residuals vs. Fitted")
```

A qualitative inspection of the plots confirms that these assumptions are met to a satisfactory degree. The validation of the model and the lack of significant interaction between the factors confirm that the indicated contribution of the `rate` factor is indeed representative for its greater effect on the response value `hemoglobin`.

The insignificance of the interaction also allows the use of an additive model, which allows more specific comparisons between the factor levels. The `summary` function is used to extract the coefficients of the model.

```{r}
model2 <- lm(hemoglobin~rate+method, data=hg_data); invisible(anova(model2)); summary(model2)
```

Due to the treatment parameterization, the first level of both factors is used as a reference level, so the contributions $\alpha_1$ and $\beta_1$ contributions are equal to zero. As the hemoglobin levels can be seen as the sum $Y_{i,j}=\mu+\alpha_i+\beta_j+e_{j,k}$, the highest yield can be calculated by adding the highest $\alpha_i$ and $\beta_j$ contributions to the mean $\mu$. From the summary it is evident that these are rate 2 (5g dose) and method B. On the other hand, when extracting the `rate` and `method` factors for the highest hemoglobin measurement in the data set, the other method is present.: `r hg_data[which.max(hg_data$hemoglobin),]` However, this does not show contradiction, as the `method` factor is not significant in this case. Rate 2, on the other hand, is confirmed as the highest yielding rate, resulting in a mean hemoglobin level of `r mu + summary(model2)$coefficients[2,1] + summary(model2)$coefficients[5,1]`.

Another sample measurement is the mean hemoglobin yield under a combination of rate 3 and method A. This can be calculated by adding the contributions $\alpha_3$ and $\beta_1$ (which is zero due to treatment parameterization) to the mean $\mu$ and obtaining `r mu + summary(model2)$coefficients[3,1]`.

**d)** Ignoring the factor `method`, the effect of the different `rate` levels can be examined in more detail using a one-way ANOVA to test the null-hypothesis that hemoglobin levels are the same for all levels.

```{r}
model3 <- lm(hemoglobin~rate, data=hg_data); anova(model3); summary(model3)
```

Once again, the p-value of the `rate` factor is below the significance level, indicating a rejection of the null-hypothesis. The estimated hemoglobin values for each rate are obtained from the summary: for rate 1: `r summary(model3)$coefficients[1,1]`, for rate 2: `r mu + summary(model3)$coefficients[2,1]`, for rate 3: `r mu + summary(model3)$coefficients[3,1]` and for rate 4: `r mu + summary(model3)$coefficients[4,1]`. Again, rate 2 is identified as the highest yielding one.

Whether this test is valid, remains to be seen by checking the assumptions of the ANOVA test. The residuals of the model are examined for normality using a Q-Q-plot and are plotted against the fitted model values.

```{r fig.height = 3.2, fig.width = 6, fig.align = "center"}
par(mfrow=c(1,2))
qqnorm(residuals(model3)); plot(fitted(model3), residuals(model3)); title("Residuals vs. Fitted")
```

Even though a linear relationship between the theoretical and sample quantiles is visible, the tails show a more extreme deviation the QQ-plot of the two-way ANOVA, casting doubts on the normality assumption. This could be problematic for the correct interpretation of the test output.

**e)** As an alternative, a Kruskal-Wallis test is performed to test the same null-hypothesis, as it operates with ranks and does not assume normality.

```{r}
kruskal.test(hg_data$hemoglobin, hg_data$rate)
```

The resulting p-value is far below the significance level, indicating a rejection of the null-hypothesis and confirming the indication from the one-way ANOVA test that the hemoglobin levels differ across the rate levels. Given the questionable normality of the residuals, the Kruskal-Wallis test is a more robust alternative. However, it provides less detailed insight into the differences between the rate levels, as it uses a more generalised test statistic about the group medians and does not involve the more sophisticated model fitting of ANOVA.

## Exercise 3. Sour Cream

**a)** We perform a three-way ANOVA using treatment parameterization. To ensure our ANOVA results are applicable, we inspect the QQ plot as well as fitted vs. residual plot to test for normality visually.

```{r, fig.height=5, fig.width=5, results='hide', fig.align = "center"}
data <- read.csv("data/cream.txt", sep="", header=TRUE)
cream_treatment <- data.frame(acidity=data[,1], batch=factor(data[,2]), 
                            position=factor(data[,3]), starter=factor(data[,4]))

creamaov_treatment <- lm(acidity ~ batch + position + starter, 
                         data=cream_treatment)

# Inspection of data
summary(data)
par(mfrow=c(2,2))
boxplot(data, main="Dataset Boxplot")

# Visual normality check
hist(data$acidity, xlab="Acidity", main="Acidity Histogram")
qqnorm(residuals(creamaov_treatment))
plot(fitted(creamaov_treatment), residuals(creamaov_treatment), main="Fitted vs. Residuals")

anova(creamaov_treatment); summary(creamaov_treatment)

starter2_p_value <- summary(creamaov_treatment)$coefficients[10,4]
```

We pass this normality test based on the near-linear distribution in the Q-Q plot and the lack of patterns visible in the fitted vs. residuals plot. We then proceed to inspect ANOVA coefficients to determine the significance of our baseline scenario $\mu_1 = \mu + \alpha_1 + \beta_1 + \gamma_1 = \mu$ and the mean of level **starter2** $\mu_2 = \mu + \alpha_1 + \beta_1 + \gamma_2 = \mu + \gamma_2$ sharing the same mean $\mu_1 = \mu_2$ i.e. $\gamma_2 = 0$. As the p-value of this coefficient is `r round(starter2_p_value, 5)` and larger than $\alpha = 0.05$, we state that there is no significant difference between the means of the two starters on acidity and assume they stem from the same distribution.

**b)** As we have no traditional baseline scenario in our dataset, we will be using the sum parameterization for all following ANOVA tests as these lend themselves to be more intuitive coefficients for our analysis.

```{r, results='hide'}
cream_sum <- data.frame(acidity=data[,1], batch=factor(data[,2]), 
                        position=factor(data[,3]), starter=factor(data[,4]))
contrasts(cream_sum$batch) <- contr.sum
contrasts(cream_sum$position) <- contr.sum
contrasts(cream_sum$starter) <- contr.sum

creamaov_sum <- lm(acidity ~ batch + position + starter, data=cream_sum)
anova(creamaov_sum); summary(creamaov_sum)

p_value_batch <- anova(creamaov_sum)$'Pr(>F)'[1]
p_value_position <- anova(creamaov_sum)$'Pr(>F)'[2]
p_value_starter <- anova(creamaov_sum)$'Pr(>F)'[3]
p_value_batch2 <- summary(creamaov_sum)$coefficients[3,4]
```
Looking at the sum parameterization ANOVA table using the full model, we find that the p-value for the factor *position* is `r round(p_value_position, 5)` and all its individual levels have coefficients corresponding to a p-value larger than `0.05`, stating a lack of significance as main effect for the model. Due to the small size of the dataset, we are unable to thoroughly test for interactions, thus we will assume them to be negligible from here on. In this case, we can remove the variable *position* from the model to simplify.

As the *batch* factor has a p-value of `r round(p_value_batch, 5)` below $\alpha = 0.05$ we can conclude that it is significant for the model and should be kept in our fixed effects model, further substantiated by a particularly deviating instance of **batch2** with p-value `r round(p_value_batch2, 5)`.

The resulting fixed effects model formula therefore becomes `acidity ~ starter + batch` ignoring interactions and its analysis can be found below.

```{r, fig.height=3.2, fig.width=6, results='hide', fig.align = "center"}
creamaov_fixed_sum <- lm(acidity ~ batch + starter, data=cream_sum)

# Visual normality re-checking
par(mfrow=c(1,2))
qqnorm(residuals(creamaov_fixed_sum))
plot(fitted(creamaov_fixed_sum), residuals(creamaov_fixed_sum)); title("Residuals vs. Fitted")

anova(creamaov_fixed_sum); summary(creamaov_fixed_sum)
```
After passing the somewhat skewed but still rather normal distribution in the Q-Q plot, using this model in the ANOVA analysis we find that coefficients for **starter3** and **starter4** substantiate a very significant effect on *acidity* with confidence level below $\alpha^{***} = 0.001$. To further substantiate our results, we will use the algorithmic step function and compare if its resulting model is consistent with our manual assessment.

```{r, fig.height=3.2, fig.width=6, results='hide', fig.align = "center"}
creamaov_sum_auto <- step(creamaov_sum)

# Visual normality re-check
par(mfrow=c(1,2))
qqnorm(residuals(creamaov_sum_auto))
plot(fitted(creamaov_sum_auto), residuals(creamaov_sum_auto)); title("Residuals vs. Fitted")

anova(creamaov_sum_auto); summary(creamaov_sum_auto)
step_formula <- formula(creamaov_sum_auto)
```

Again passing the normality check, utilizing the step function utility in its default configuration further agrees with our previous results as the algorithm arrives at the final formula `r step_formula` thus also removing the insignificant block variable *position* from the model. The resulting model using this algorithm is identical to the one we established manually above.

**c)** Since each combination of *batch* and *starter* exists once in the dataset, we can apply the Friedman test to test whether there is an effect of *starter* on *acidity*. The Friedman test does not assume normality so it should be more robust, albeit less sensitive than the ANOVA performed before.

```{r}
friedman_test <- friedman.test(acidity ~ starter | batch, data=cream_sum)
p_value_friedman <- friedman_test$p.value
```

The p-value of the Friedman test is `r round(p_value_friedman, 5)` which is lower than the confidence level of $\alpha = 0.05$ and therefore we reject the null hypothesis that the means stem from the same distribution and instead agree with prior results that there is a significant effect of *starter* on *acidity*.

**d)** A mixed-effect analysis is performed using the library `lme4`:

```{r, fig.height=5, fig.width=5, results='hide', warning=FALSE, message=FALSE, fig.align = "center"}
library(lme4)

# Check if starter has an effect on acidity
mixedaov_1_pos <- lmer(acidity ~ batch + starter + (1|position), data=cream_sum)
mixedaov_2_pos <- lmer(acidity ~ batch + (1|starter) + (1|position), data=cream_sum)

# Visual normality check
par(mfrow=c(2,2))
qqnorm(residuals(mixedaov_1_pos))
plot(fitted(mixedaov_1_pos), residuals(mixedaov_1_pos)); title("Residuals vs. Fitted")
qqnorm(residuals(mixedaov_2_pos))
plot(fitted(mixedaov_2_pos), residuals(mixedaov_2_pos)); title("Residuals vs. Fitted")

# Remove position to simplify
mixedaov_1 <- lmer(acidity ~ starter + (1|batch), data=cream_sum)
mixedaov_2 <- lmer(acidity ~ (1|starter) + (1|batch), data=cream_sum)

anova(mixedaov_2_pos, mixedaov_1_pos)
anova(mixedaov_2, mixedaov_1)
p_signif_diff_pos <- anova(mixedaov_2_pos, mixedaov_1_pos)$'Pr(>Chisq)'[2]
p_signif_diff <- anova(mixedaov_2, mixedaov_1)$'Pr(>Chisq)'[2]
```

In the Q-Q plot, we seem to have stronger skewing of the distribution but taking into account slight outliers, it still roughly follows a normal distribution. Using the **`(1|position)`** notation, we allow the intercepts of the model to vary randomly across different levels of the factor *position* in our model. Comparing the mixed model with the factor *starter* included conventionally versus as random effect using ANOVA we find a p-value of `r round(p_signif_diff, 5)` which is significant thus we confirm our previous ANOVA results and assumptions that *starter* is a relevant factor for *acidity*. The results are similar with a p-value of `r round(p_signif_diff_pos, 5)` when removing the insignificant factor *position* from the model.
