---
title: "Assignment 2 - Exercise 2"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## Exercise 2. Birthweights

This exercise explores the data set `Birthweight.csv` which contains information on new born babies and their parents. A first examination reveals the 16 variables with 42 observations:
```{r}
birthweight <- read.csv("data/Birthweight.csv"); str(birthweight)
```
For the first part of the analysis, the variables `ID`, `smoker`, `lowbwt` and `mage35` are disregarded, the column `Birthweight` is selected as a response variable, while the other 11 variables are considered predictors.
```{r}
birthweight1 <- birthweight
birthweight1$ID <- NULL; birthweight1$smoker <- NULL
birthweight1$lowbwt <- NULL; birthweight1$mage35 <- NULL
```

**a)** The explanatory variables `Length`, `Headcirk`, `Gestation`, `mage`, `mnosig`, `mheight`, `mppwt`, `fage`, `fedyrs`, `fnosig`, and `fheight` are examined for potential (leverage) points. In case such are found, it is to be verified whether these are influence points by examining the effect of their removal. A qualitative investigation through plots is possible, but a more compact and quantitative approach is to calculate Cook's distance for each observation within each explanatory variable. A Cook's distance of a potential point larger than one provides evidence that this point is in fact an influence point. The following code iterates through all predictors and prints the maximum Cook's distance among the observations.

```{r}
# Create data frame with Cook's distances
cdist_df <- data.frame()
for (i in 1:length(birthweight1)) {
  if (names(birthweight1)[i] == "Birthweight") next
  bw_model <- lm(Birthweight~birthweight1[,i], data=birthweight1)
  cdist <- cooks.distance(bw_model)
  cdist_df <- rbind(cdist_df, data.frame(Variable = names(birthweight1)[i],
                                         CooksDistance = max(cdist)))
}
print(t(cdist_df))
```

None of the resulting Cook's distances are sufficiently large to conclude that the points are influence points. Another potential problem to be addressed is collinearity between the explanatory variables. A preliminary investigation can be conducted by analysing the variance inflation factors (VIF) of a model including the 11 predictors.
```{r warning=FALSE, message=FALSE}
bw_model <- lm(Birthweight~., data=birthweight1); library(car); vif(bw_model)
```

In general, the resulting VIF values are not large enough to indicate collinearity. Nonetheless, `mage` and `fage`, have VIF values close to 5. Since the above analysis does not provide information about the collinear groups that these two belong to, a more detailed examination can be performed on a larger selection of variables, for instance the ones with a VIF larger than 2: `Length`, `Gestation`, `mage`, `mheight`, `mppwt` and `fage`. To examine their correlations visually, a pairwise scatter plot is created.
```{r fig.height = 4, fig.width = 6, fig.align = "center"}
pairs(birthweight1[,c("Length", "Gestation", "mage", "mheight", "mppwt", "fage")])
```
Linear correlations are observed in pairs like `Length` and `Gestation`, `mage` and `fage`, and `mheight` and `mppwt`. The first one is indicative of the fact that the length of the gestation period is correlated to the baby's growth. The second one is expected, as the mother's and father's age commonly do not differ a lot. The third one is also logical, as the mother's height and pre-pregnancy weight are likely to be correlated. The presence of collinearity is not a problem in this case, as the VIF values are not large enough to indicate that the estimates of the coefficients are unstable.

**b)** To reduce the number of explanatory variables, the step-down method is applied by iteratively analysing the significance of the influence of all independent variables on `Birthweight` and removing the least significant one, then repeating the process until all variables have a significant effect.
```{r}
model_summary <- summary(bw_model)
p_values <- model_summary$coefficients[,4]
birthweight2 <- birthweight1
while (max(p_values) > 0.05) {
  max_p_name <- names(which.max(p_values))
  birthweight2 <- birthweight2[, names(birthweight2) != max_p_name]
  bw_model <- lm(Birthweight~., data=birthweight2)
  model_summary <- summary(bw_model)
  p_values <- model_summary$coefficients[,4]
}
model_summary
```
The final model is reduced to only two explanatory variables: `Headcirc` and `Gestation`, both of which exhibit a p-value below 0.05. Since the head circumference can be considered indicative of the baby's size, and the length of the gestation period clearly determines the baby's growth in size prior to birth, the influence of the two variables seems logical.

**c)** For the next exercise, the average of each predictor value from the reduced model is taken and used as a new observation, for which the 95% confidence and prediction intervals for the response variable `Birthweight` are calculated.
```{r}
averages <- data.frame(Headcirc=mean(birthweight2$Headcirc), Gestation=mean(birthweight2$Gestation))
stepdown_ci <- predict(bw_model, newdata=averages, interval="confidence", level=0.95)
stepdown_pi <- predict(bw_model, newdata=averages, interval="prediction", level=0.95)
```
The resulting fitted value for `Birthweight` is `r stepdown_ci[,1]`. As expected, the prediction interval [`r stepdown_pi[,2]`, `r stepdown_pi[,3]`] is wider than the confidence interval [`r stepdown_ci[,2]`, `r stepdown_ci[,3]`], as it encompasses individual observations instead of observation means and thus accounts for the error in these observations as well.

**d)** As an alternative to the step-down method, the LASSO method is applied to the original model to reduce the number of explanatory variables. The `cv.glmnet` function is used to select the optimal value of the tuning parameter $\lambda$ by cross-validation. Like in b), the reduction starts with the filtered data set `birthweight1`. $2/3$ of the data points are randomly sampled for training and the remaining $1/3$ is reserved for subsequent testing.
```{r fig.height = 5.5, fig.width = 6, fig.align = "center", warning=FALSE, message=FALSE}
library(glmnet); par(mfrow=c(2,1), mar=c(4,4,6,1)); set.seed(123)
x_df <- birthweight1[, names(birthweight1) != 'Birthweight']
x <- as.matrix(x_df); y <- as.double(birthweight1$Birthweight)
train <- sample(1:nrow(x), 0.67*nrow(x))
x_train <- x[train,]; y_train <- y[train]
x_test <- x[-train,]; y_test <- y[-train]; x_test_df <- x_df[-train,]
lasso_model <- glmnet(x_train, y_train, alpha=1)
cv_lasso <- cv.glmnet(x_train, y_train, alpha=1, type.measure="mse")
plot(cv_lasso, main="Cross-validated MSE vs. log(lambda)")
plot(cv_lasso$glmnet.fit, xvar="lambda", label=T, main="LASSO coefficients vs. log(lambda)")
lambda_min <- cv_lasso$lambda.min; lambda_1se <- cv_lasso$lambda.1se
coef(cv_lasso, s=cv_lasso$lambda.min)
lasso_pred <- predict(lasso_model, s=lambda_min, newx=x_test)
mse_lasso <- mean((lasso_pred-y_test)^2)
stepdown_pred <- predict(bw_model, newdata=x_test_df)
mse_stepdown <- mean((stepdown_pred-y_test)^2)
```

The plot of the cross-validated MSE shows for which value of the free parameter $\lambda$ the penalty term $\lambda P(\beta)$ compensates the RSS term $\frac{1}{N}||Y-X\beta||^2$ the best. The left vertical line indicates the value of `lambda.min`, which is the value for which the cross-validated error is minimised. The right vertical line marks the value of `lambda.1se`, which is the largest value of $\lambda$ such that the error is within one standard error of the minimum. The second plot depicts the shrinkage of the coefficients with increasing $\lambda$. Since this shrinkage means that the model is becoming simpler, `lambda.1se` is relevant for finding the optimal trade-off between a minimum MSE and a most simplified model.

When observing the coefficients of the simplified model, it is noticeable that `Headcirc` and `Gestation` are present, just like in the reduced model from the step-down method. The rest of the remaining coefficients in the LASSO model, `Length`, `mheight` and `fheight` are close to zero but still present. Both the LASSO-optimised model and the model from b) are tested with the remaining subset of one third of the data points. This results in a MSE of `r mse_lasso` for the LASSO method and a MSE of `r mse_stepdown` for the step-down method. Surprisingly, the LASSO method does not seem to perform better than the step-down method in this case. Nonetheless, its merit may lie in its efficient handling of models of much higher complexity.

**e)** The next exercises consider a new subset of the data set: the binary variable `lowbwt`, indicating whether the newborn baby's weight is under 6 lbs, is used as a response variable, while the variables `Gestation` (gestation period), `smoker` (a binary variable indicating whether the mother is a smoker) and `mage35` (a binary variable indicating whether the mother's age is above 35) are used as predictors. A new subset of the data set is created with only the relevant contents. Two bar plots are created to illustrate the relationship between each of the binary predictors and the binary outcome.
```{r fig.height = 3, fig.width = 6, fig.align = "center", warning=FALSE, message=FALSE}
library(dplyr); par(mfrow=c(1,2))
birthweight3 <- select(birthweight, lowbwt, Gestation, smoker, mage35)
#birthweight3$smoker <- factor(birthweight3$smoker)
#birthweight3$mage35 <- factor(birthweight3$mage35)
totsmoker <- xtabs(~smoker, data=birthweight3)
barplot(xtabs(lowbwt~smoker, data=birthweight3), xlab="Smoker", ylab="Number of Low Birthweights")
totmage35 <- xtabs(~mage35, data=birthweight3)
barplot(xtabs(lowbwt~mage35, data=birthweight3), xlab="Age > 35", ylab="Number of Low Birthweights")
tot <- xtabs(~smoker+mage35, data=birthweight3)
totlowbwt <- xtabs(lowbwt~smoker+mage35, data=birthweight3)
print(totsmoker); print(totmage35); print(round(totlowbwt/tot, 2))
```
Among the observations, only a single non-smoking mother gave birth to an underweight baby. Looking at the age, only one mother older than 35 gave birth to an underweight baby. As the tabular summary of the variable `smoker` suggests, the distribution of smoking vs. non-smoking mothers is quite even, so the prevalence of smokers in low birth weights can be considered indicative of the claim that smoking mothers have lighter babies. On the other hand, there are only 4 35+ mothers in the data set, so the proportions of light babies per age category needs to be considered: 1 in 4 older mothers have a light baby, whereas this applies for only $5/38\approx 0.13$ of the younger mothers. This may be taken as an indication that a low birth weight occurs in a higher percentage with older mothers. A table indicating the proportion of low birth weights for each combination of `smoker` and `mage35` suggests that the highest occurrence of low birth weights is among smoking mothers older than 35 - low birth weight applies to one third of this group. 

**f)** The relationship between the response variable and the predictors is further investigated by fitting a logistic regression model without interactions to the data.
```{r}
bw_logmodel <- glm(lowbwt~Gestation+smoker+mage35, data=birthweight3, family="binomial")
summary(bw_logmodel)
```
The summary of the model provides the coefficients needed to estimate the odds ratios for the predictors using the formula $\hat{o_k}=e^{\mu+c_1g_k+c_2s_k+c_3m_k}$, where $g_k$, $s_k$ and $m_k$ are the $k$-th observation of `Gestation`, `smoker` and `mage35`, respectively, and $c_1$, $c_2$ and $c_3$ are their coefficients. The coefficient for `Gestation` is negative, which means that the odds of a low birth weight decrease with increasing gestation period, namely by a factor $e^{-1.4633}\approx 0.231$. The coefficients for `smoker` and `mage35` are positive, which means that the odds of a low birth weight increase for smoking mothers and mothers older than 35. This happens by a factor $e^{5.4495}\approx 232.64$ for smoking mothers and by a factor $e^{0.3223}\approx 1.38$ for older mothers. This confirms the previous observations that older age and, more primarily, smoking are associated with a higher risk of low birth weight

**g)** Here, we follow a similar procedure to that in f), but we include the respective interactions in the models. Then we can run an ANOVA to determine if the interaction term is significant. 
```{r}
bw_logmodel_int1 <- glm(lowbwt~Gestation+smoker+mage35+Gestation:smoker, data=birthweight3, family="binomial")
bw_logmodel_int2 <- glm(lowbwt~Gestation+smoker+mage35+Gestation:mage35, data=birthweight3, family="binomial")
anova(bw_logmodel_int1, test="Chisq")
anova(bw_logmodel_int2, test="Chisq")
```
We notice that, for both models where the interaction term was tested individually, neither interaction term was significant according to our ANOVA. However, both the variables `Gestation` and `smoker` are significant, and should be included in the model despite not having an interaction. In this case, we would prefer to use the model from f) and not g).  

```{r, fig.height = 3, fig.width = 6}
par(mfrow=c(1,2))
qqnorm(residuals(bw_logmodel_int1), main="Q-Q Plot Smoker Interact")
qqnorm(residuals(bw_logmodel_int2), main="Q-Q Plot mage35 Interact")
```
It is also a good idea to check our assumptions for the ANOVA test as well. We see that our model residuals are not quite linear in the Q-Q Plot, so normality is doubtful. This is further proof that using the model with interactions not well founded, and should proceed with our model from part f) with no interactions. 

**h)** Our resulting model from f) will be used below to determine the probability of low birth weight for the combinations of factors with a gestation length of 40 weeks. 
```{r}
#final_logmodel <- glm(lowbwt~Gestation+smoker, data=birthweight3, family="binomial")
combo1 = data.frame(Gestation=40, smoker=0, mage35=0)
combo2 = data.frame(Gestation=40, smoker=1, mage35=0)
combo3 = data.frame(Gestation=40, smoker=0, mage35=1)
combo4 = data.frame(Gestation=40, smoker=1, mage35=1)
plbw_combo1 = predict(bw_logmodel,combo1,type="response")
plbw_combo2 = predict(bw_logmodel,combo2,type="response")
plbw_combo3 = predict(bw_logmodel,combo3,type="response")
plbw_combo4 = predict(bw_logmodel,combo4,type="response")
summary_table <- matrix(c(plbw_combo1, plbw_combo3, plbw_combo2, plbw_combo4), 
                        byrow = TRUE, 
                        ncol = 2, 
                        dimnames = list(c("Non-smoker", "Smoker"), 
                                        c("Under 35", "Over 35")))
print(summary_table)
```
The results from the probability estimation  are summarized in the above table. We notice once again that the probability of having a baby with a low birth weight is highest for a mother that is over 35  and a smoker, and lowest for a mother that is below 35 and not a smoker, which follows our intuition. 

**i)** In order to perform a contingency table test, the dataframe needs to be converted into a matrix. 
```{r}
#Creating the matrix for the smoker boolean low birthweight
grouped_df_smoker <- with(birthweight3, table(lowbwt, smoker))
dims <- dim(grouped_df_smoker)
flat_table <- as.vector(grouped_df_smoker)
matrix_data_smoker <- matrix(flat_table, nrow = prod(dims[-1]), ncol = dims[1])
rownames(matrix_data_smoker) <- c("lowbwt=0", "lowbwt=1")
colnames(matrix_data_smoker) <- c("smoker=0", "smoker=1")

#Creating the matrix for the mage35 boolean low birthweight
grouped_df_mage35 <- with(birthweight3, table(lowbwt, mage35))
dims <- dim(grouped_df_mage35)
flat_table <- as.vector(grouped_df_mage35)
matrix_data_mage35 <- matrix(flat_table, nrow = prod(dims[-1]), ncol = dims[1])
rownames(matrix_data_mage35) <- c("lowbwt=0", "lowbwt=1")
colnames(matrix_data_mage35) <- c("mage35=0", "mage35=1")

#Conducting the two Fisher tests since we have two 2x2 matrices
pval_smoker <- fisher.test(matrix_data_smoker)[[1]]
pval_mage35 <-  fisher.test(matrix_data_mage35)[[1]]
```

Since both of our p-values, `r pval_smoker`, `r pval_mage35` are above 0.05, we would not reject the null hypothesis that there is neither a dependence between smoking and low birth weight nor a dependence between `mage35` and low birth weight. There is certainly an issue with using this method. To start, we do not have at least 5 observations for each combination, which will make our test unreliable. However, the approach in general is not incorrect. One advantage to using a chi-square test is that it simplifies the problem to mere dependence vs independence between variables. However, it does not give specific coefficient variables to delinate how the probabilities of events occurring change with respect to the categorical variables, nor does it allow for continuous variables such as `Gestation` to be taken into account. 